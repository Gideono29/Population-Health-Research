# -*- coding: utf-8 -*-
"""SAT_5114_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s_LOypnMIwExs7JN5L5KsdzCr-XUT34D

#SAT 5114
#AI in HEALTHCARE PROJECT

##Install and load libraries
"""

!pip install -qqq "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git" --progress-bar off
!pip install -qqq xformers trl peft accelerate bitsandbytes triton --progress-bar off
!pip install -qqq unsloth transformers accelerate datasets peft bitsandbytes wandb evaluate bert-score

"""##Import Libraries"""

# Import libraries
import torch
import unsloth
import numpy as np
from tqdm import tqdm
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    TextStreamer,
    LogitsProcessor,
    LogitsProcessorList,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model
from sklearn.metrics import f1_score
import evaluate

"""##Install and define evaluation"""

!pip install rouge_score
bleu_metric = evaluate.load("bleu")
rouge_metric = evaluate.load("rouge")
bertscore_metric = evaluate.load("bertscore")

"""##Load and prepare the dataset"""

dataset = load_dataset("Shekswess/gemma_medquad_instruct_dataset", split="train[:400]")
dataset = dataset.train_test_split(test_size=0.1)

#lavita/ChatDoctor-HealthCareMagic-100k
#Shekswess/gemma_medquad_instruct_dataset

"""##Format the dataset"""

# Formatting function
def format_instruction(example):
    return {
        "prompt": f"### Instruction:\n{example['instruction']}\n\n### Input:\n{example['input']}\n\n### Response:\n",
        "response": example["output"]
    }

dataset = dataset.map(format_instruction)

"""##Model setup and quantization"""

# Model setup with 4-bit quantization
model_name = "unsloth/llama-3-8b"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto",
    use_cache=False,
)

"""#LoRA configuration and PEFT"""

# Improved LoRA configuration
peft_config = LoraConfig(
    r=4,
    lora_alpha=8,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="lora_only",
    task_type="CAUSAL_LM",
    inference_mode=False,
)
model = get_peft_model(model, peft_config)
model.enable_input_require_grads()

"""##Data Collation and Tokenization"""

# Add data collator
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    pad_to_multiple_of=8,
    padding=True,
    return_tensors="pt",
)

# Update tokenize function
def tokenize_function(examples):
    texts = [p + r for p, r in zip(examples["prompt"], examples["response"])]
    tokenized = tokenizer(
        texts,
        max_length=512,
        truncation=True,
        padding="max_length",
        add_special_tokens=False
    )
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": tokenized["input_ids"].copy()
    }

tokenized_dataset = dataset.map(tokenize_function, batched=True)

"""##Metrics calculation"""

# Metrics calculation (updated)
def compute_metrics(eval_pred):
    preds, labels = eval_pred
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    # Convert logits to token IDs (shape: [batch_size, seq_length])
    preds = np.argmax(preds, axis=-1)  # Add this line

    pred_texts = tokenizer.batch_decode(preds, skip_special_tokens=True)
    label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)

    results = {}

    # BLEU
    results["bleu"] = bleu_metric.compute(
        predictions=pred_texts,
        references=[[text] for text in label_texts]
    )["bleu"]

    # ROUGE
    results.update(rouge_metric.compute(
        predictions=pred_texts,
        references=label_texts
    ))

    # BERTScore
    bert_results = bertscore_metric.compute(
        predictions=pred_texts,
        references=label_texts,
        lang="en"
    )
    results["bert_score"] = np.mean(bert_results["f1"])

    return results

"""##Setup Training arguments and trainer"""

# Modified TrainingArguments with evaluation strategy
training_args = TrainingArguments(
    output_dir="./llama3_healthcare",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=1,
    save_strategy ='epoch',      # Changed to "epoch" to enable saving
    logging_strategy="no",     # Disable logging
    learning_rate=1e-5,
    weight_decay=1,
    num_train_epochs=4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.1,
    logging_steps=50,
    load_best_model_at_end=True,
    fp16=True,
    gradient_checkpointing=True,
    report_to="none",        # Disabled all reporting
    remove_unused_columns=False,
    # Add evaluation strategy for EarlyStoppingCallback
    eval_strategy = "epoch"  # or "steps" with logging_steps defined
)

# Trainer remains the same
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Training will now only log to console
trainer.train()

"""##Model saving"""

# Save and generation code remains unchanged
model.save_pretrained("./llama3_healthcare_finetuned")
tokenizer.save_pretrained("./llama3_healthcare_finetuned")

"""##Text generation"""

from transformers import pipeline # Import the pipeline function
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,

    max_new_tokens=256,
    do_sample=True,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.2,
)

test_question = "What is the management for hypertension?"
result = generator(test_question, num_return_sequences=1)
print("\nGenerated Response:")
print(result[0]['generated_text'])



"""#Add Graphical User Interface for input and output texts

##Install GUI library gradio
"""

!pip install gradio

"""##Define predicition Function"""

import gradio as gr

def predict(input_text):
  result = generator(input_text, num_return_sequences=1)
  return result[0]['generated_text']

"""#Create the user interface"""

iface = gr.Interface(
    fn=predict,
    inputs=gr.Textbox(lines=2, placeholder="Enter your question here..."),
    outputs="text",
    title="Medical Question Answering",
    description="Ask questions about medical topics and get answers from our AI model."
)

iface.launch()